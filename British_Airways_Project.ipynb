{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfe670ad-fc87-4f0c-a9e4-e438730b6c24",
   "metadata": {},
   "source": [
    "### British Airways Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831bad96-a88f-423e-9bf4-d457603729e9",
   "metadata": {},
   "source": [
    "## Overview \n",
    "The goal of this project is to learn about British Airways customers reviews of the airline. \n",
    "I will scrap a review website to look at how their customers reviewd the airline and try and \n",
    "gain insights to understand the customers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f38e619-a394-49f3-8956-7ecaebb53f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages \n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import nltk.corpus\n",
    "from nltk.tokenize import word_tokenize \n",
    "import seaborn as sns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a71569-ba5d-42ec-94a9-d2a53ec24b0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scrapping page 1\n"
     ]
    }
   ],
   "source": [
    "base_url = 'https://www.airlinequality.com/airline-reviews/british-airways'\n",
    "pages = 20\n",
    "page_size = 100\n",
    "\n",
    "reviews = []\n",
    "\n",
    "for i in range(1, pages + 1):\n",
    "    print(f'scrapping page {i}')\n",
    "    \n",
    "    # Create URL to collect links from paginated data\n",
    "    url = f\"{base_url}/page/{i}/?sortby=post_date%3ADesc&pagesize={page_size}\"\n",
    "    \n",
    "    # Collect HTML data from this page\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Parse content\n",
    "    content = response.content\n",
    "    parsed_content = BeautifulSoup(content, 'html.parser')\n",
    "    for para in parsed_content.find_all(\"div\", {\"class\": \"text_content\"}):\n",
    "        reviews.append(para.get_text())\n",
    "        \n",
    "        \n",
    "    print(f\"   ---> {len(reviews)} total reviews\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd16a598-31aa-4886-9420-113af993f66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df[\"reviews\"] = reviews\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869eccbc-789a-4417-8eef-b433aa1f00d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd17b511-37b5-4737-b9c6-f170c4c68a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"BA_reviews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f434912a-fa2b-496a-8566-2d00c6976fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df:\n",
    "    df['reviews'] = df['reviews'].map(lambda x: x.replace('✅ Trip Verified |',''))\n",
    "    df['reviews'] = df['reviews'].map(lambda x: x.replace('Not Verified |',''))\n",
    "    df['reviews'] = df['reviews'].map(lambda x: x.replace('✅ Verified Review | ',''))\n",
    "    df['reviews'] = df['reviews'].map(lambda x: x.replace('.',''))\n",
    "    df['reviews'] = df['reviews'].map(lambda x: x.replace(',',''))\n",
    "    df['reviews'] = df['reviews'].map(lambda x: x.replace('\"',''))\n",
    "    df['reviews'] = df['reviews'].map(lambda x: x.replace('?',''))\n",
    "    df['reviews'] = df['reviews'].map(lambda x: x.replace('!',''))\n",
    "    df['reviews'] = df['reviews'].map(lambda x: x.replace('(',''))\n",
    "    df['reviews'] = df['reviews'].map(lambda x: x.replace(')',''))\n",
    "    df['reviews'] = df['reviews'].map(lambda x: x.replace('n\\'t',' not'))\n",
    "    df['reviews'] = df['reviews'].map(lambda x: x.replace('-',' not'))\n",
    "    df['reviews'] = df['reviews'].map(lambda x: x.replace('’',''))\n",
    "    df['reviews'] = df['reviews'].map(lambda x: x.replace('get',''))\n",
    "    df['reviews'] = df['reviews'].map(lambda x: x.replace('would',''))\n",
    "    df['reviews'] = df['reviews'].map(lambda x: x.replace('one',''))\n",
    "    df['reviews'] = df['reviews'].map(lambda x: x.replace('ba','british-airways'))\n",
    "    df['reviews'] = df['reviews'].map(lambda x: x.replace('notairways','not airways'))\n",
    "    df['reviews'] = df['reviews'].map(lambda x: x.replace('British Airways','british-airways'))\n",
    "    df['reviews'] = df['reviews'].map(lambda x: x.lower())\n",
    "    df.to_csv('BA_reviews_cleaned.csv',index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb25cf61-cdf2-420f-856f-afcc65a5e573",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27977090-8f09-4573-960b-53389dfdfb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('BA_reviews_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f3eca4-8457-4613-a836-532032d47699",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9598a769-fd9e-4439-b86a-7f36f95513d6",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5a60e2-ffc4-4d66-b207-c32d03cf3f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3e3f90-38d8-405e-83c1-1ffe5e5e572d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of revies from the DataFrame \n",
    "reviews_list = list(df['reviews'])\n",
    "reviews_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa2fab2-9efa-412d-ad4a-6d3758864cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list to store the scores in\n",
    "scores_combined = []\n",
    "# itterate through the reviews to score the sentimate of the review. Print the review and the scores for a quick check. \n",
    "for reviews in reviews_list:\n",
    "    scores = analyzer.polarity_scores(reviews)\n",
    "    print(reviews)\n",
    "    print(scores)\n",
    "    # add the scores to the list\n",
    "    scores_combined.append(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf680583-9170-4000-95e6-b770d8277df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scores_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954af069-6891-4673-ba38-6ae17c55cb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a DataFrame of the scores\n",
    "df_scores = pd.DataFrame(scores_combined)\n",
    "df_scores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b0cac2-e208-4e8a-a9f4-09895fe70452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# join the dataframe scores to the original dataframe\n",
    "df = df.join(df_scores)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ee014f-4c43-4cc9-9e30-8562def1d789",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f99779-d29e-49e9-8bb3-b8c87be516ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb2505f-fa19-49d0-a02e-e6bc7e899cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(column):\n",
    "    \"\"\"\n",
    "    Tokenizes a Pandas dataframe column and returns a list of tokens.\n",
    "\n",
    "    Args:\n",
    "        column: Pandas dataframe column (i.e. df['text']).\n",
    "\n",
    "    Returns:\n",
    "        tokens (list): Tokenized list, i.e. [word, word, word]\n",
    "    \"\"\"\n",
    "\n",
    "    tokens = nltk.word_tokenize(column)\n",
    "    return [w for w in tokens if w.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a26e545-c979-40fb-8168-21eec11f2dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokenized'] = df.apply(lambda x: tokenize(x['reviews']), axis=1)\n",
    "df[['tokenized']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457669d8-ef2b-4325-b08e-a04b0dad54aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26375bfe-e4f3-4ea5-bcaa-6e66550f2ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ((ax1,ax2), (ax3,ax4)) = plt.subplots(2,2)\n",
    "ax1.hist(x= df['neg'], color = 'red',)\n",
    "ax1.set_title('Polarity Scores Negative')\n",
    "ax2.hist(x=df['neu'],color = 'orange')\n",
    "ax2.set_title('Polarity Scores Neutral')\n",
    "ax3.hist(x=df['pos'],color = 'green')\n",
    "ax3.set_title('Polarity Scores Positive')\n",
    "ax4.hist(x=df['compound'],color = 'blue')\n",
    "ax4.set_title('Polarity Scores Compound')\n",
    "fig.suptitle('Sentiment Analysis Polarity Scores')\n",
    "ax1.set_ylim(ymax=800)\n",
    "ax1.set_xlim(xmax=1.0)\n",
    "ax2.set_ylim(ymax=800)\n",
    "ax2.set_xlim(xmax=1.0)\n",
    "ax3.set_ylim(ymax=800)\n",
    "ax3.set_xlim(xmax=1.0)\n",
    "ax4.set_ylim(ymax=800)\n",
    "ax4.set_xlim(xmax=1.0)\n",
    "fig.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33b64af-02c0-4cc2-9984-caa7119a1d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['compound'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583ab165-51c4-456b-aab1-ba54e96094e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## this is where I am currently working. trying to make a frequency distribution of words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3892c3e6-35ea-4c4e-8e52-62b9c287eb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text in the 'text_column'\n",
    "tokenized_text = [word.lower() for text in df['reviews'] for word in word_tokenize(str(text))]\n",
    "# remove stop words\n",
    "import nltk\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "tokenized_cleaned = [x for x in tokenized_text if x not in stopwords]\n",
    "print(tokenized_cleaned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb1b29e-9bac-40fe-bb17-0627eef2000d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "# Calculate the frequency distribution using FreqDist\n",
    "freq_dist = FreqDist(tokenized_cleaned)\n",
    "\n",
    "# Print the most common words and their frequencies\n",
    "print(freq_dist.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6705c3-27a9-409a-8f83-58822dbe7eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding the frequency distinct in the tokens\n",
    "# Importing FreqDist library from nltk and passing token into FreqDist\n",
    "from nltk.probability import FreqDist\n",
    "fdist = (FreqDist(tokenized_cleaned))\n",
    "fdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44deaa24-251f-4867-b99a-7f44bb6b3bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_fdist = pd.Series(dict(fdist.most_common(20)))\n",
    "fig, ax =plt.subplots(figsize=(10,10))\n",
    "all_plot = sns.barplot(x=all_fdist.index, y=all_fdist.values, ax=ax)\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel('Words')\n",
    "plt.ylabel('Count')\n",
    "plt.title('20 Most Common Words in Reviews');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2231d1fd-d34b-41d9-9fcb-42ff3b0dddb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d9ac83-6d18-4e99-ae96-8c06d2398ed6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8660bd91-96b9-4f20-8dca-a8bb3dd64b5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
